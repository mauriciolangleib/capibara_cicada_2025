














# cargamos las librerias
import seaborn as sns,  pandas as pd

# importamos el dataset
penguins = sns.load_dataset('penguins')

# llevamos al dataset a un dataframe de pandas
data = pd.DataFrame(penguins)


# pispeamos los datos
data.head()





# con el metodo .info() podemos ver un poco más, incluyendo datos faltantes por variable
data.info()


# con shape podemos ver las dimensiones del dataset 
print(data.shape) # cuantos individuos hay?


# con el metodo .describe() podemos obtener algunos datos mas cuantitativos de cada variable
data.describe(include='all')





# cargamos las librerias
import seaborn as sns,  pandas as pd
import matplotlib.pyplot as plt

# importamos el dataset
penguins = sns.load_dataset('penguins')

# llevamos al dataset a un dataframe de pandas
data = pd.DataFrame(penguins)


# pispeamos los datos
data.head()





# con el metodo .info() podemos ver un poco más, incluyendo datos faltantes por variable
data.info()


# con shape podemos ver las dimensiones del dataset 
print(data.shape) # cuantos individuos hay?


# con el metodo .describe() podemos obtener algunos datos mas cuantitativos de cada variable
data.describe(include='all')


sns.pairplot(data, hue="species", height=3,diag_kind="hist")





sns.scatterplot(data = data, hue = 'species', x = 'bill_length_mm', y = 'bill_depth_mm')

















from sklearn.cluster import KMeans # traemos k-means clustering de sk-learn








data_k = data.loc[data.bill_length_mm.notnull(),:].loc[data.bill_depth_mm.notnull(),:]


from sklearn.model_selection import train_test_split # separación de datos en entrenamiento, testeo y validación
X = data_k[['bill_depth_mm','bill_length_mm']].to_numpy()
y = data_k.species.to_numpy()


kmeans= KMeans(n_clusters = 3, random_state = 42)

# Compute k-means clustering
kmeans.fit(X)

# Compute cluster centers and predict cluster index for each sample.
pred = kmeans.predict(X)

pred



import matplotlib.pyplot as plt


pred





data_k = data.query("not body_mass_g.isnull() and not flipper_length_mm.isnull()")


data_k = data_r.assign(species_pred = pred)


# ----- Paleta: seaborn deep en orden azul, verde, naranja -----
deep = sns.color_palette("deep")
COLORS = [deep[0], deep[2], deep[1]]  # azul, verde, naranja

# Hardcode de colores por cluster (ajustá las claves si tus labels no son 0/1/2)
CLUSTER_COLORS = {0: COLORS[1], 
                  1: COLORS[2], 
                  2: COLORS[0]}

# Colores para etiquetas reales (mismo set, en el mismo orden determinista)
species_order = sorted(data_k['species'].astype(str).unique())
SPECIES_COLORS = {sp: COLORS[i % len(COLORS)] for i, sp in enumerate(species_order)}

# Asegurar hue categórico para evitar el problema de “2 colores”
data_k['cluster'] = pd.Categorical(data_k['species_pred'], categories=list(CLUSTER_COLORS.keys()))

fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)

# Panel A: etiqueta real
sns.scatterplot(
    data=data_k, x='bill_length_mm', y='bill_depth_mm',
    hue='species', hue_order=species_order, palette=SPECIES_COLORS,
    s=45, edgecolor=None, ax=axes[0]
)
axes[0].set_title('Etiqueta real')
axes[0].set_xlabel('Bill length (mm)')
axes[0].set_ylabel('Bill depth (mm)')
axes[0].legend(title='species')

# Panel B: predicción (clusters)
cluster_order = list(CLUSTER_COLORS.keys())
sns.scatterplot(
    data=data_k, x='bill_length_mm', y='bill_depth_mm',
    hue='cluster', hue_order=cluster_order, palette=CLUSTER_COLORS,
    s=45, edgecolor=None, ax=axes[1]
)
axes[1].set_title('Predicción (clusters)')
axes[1].set_xlabel('Bill length (mm)')
axes[1].set_ylabel('')

# Centroides (media por cluster en las dos variables ploteadas)
cent = (data_k
        .groupby('cluster')[['bill_length_mm', 'bill_depth_mm']]
        .mean()
        .reindex(cluster_order))

axes[1].scatter(
    cent['bill_length_mm'], cent['bill_depth_mm'],
    marker='*', s=250, linewidths=2, edgecolors='black',  
    c='violet', label='Centroides'
)

plt.tight_layout()
plt.show()























# calculamos la correlacion y agrupamos jerarquicamente usando el metodo .clustermap()
print('Correlacion:')
sns.clustermap(data.drop(['species', 'island','sex'], axis = 1).corr(),  
               figsize = (6,6), 
               cmap = 'vlag',
               vmax = 1, vmin = -1)








sns.scatterplot(data = data, x = 'flipper_length_mm', y = 'body_mass_g')


# vemos lo mismo segun especie?
sns.scatterplot(data = data, x = 'flipper_length_mm', y = 'body_mass_g', hue = 'species')





# y por sexo?
g = sns.FacetGrid(data, col="species", sharey = False, sharex = False)
g.map_dataframe(sns.scatterplot, x="flipper_length_mm", y="body_mass_g", hue="sex")
g.add_legend()











# guardamos ambos valores
largo_aletas = data_r.flipper_length_mm.to_numpy() * 0.1 # lo llevamos a un array de numpy con datos en cm
masa_pinguinos = data_r.body_mass_g.to_numpy() * 1e-3 # llevamos a kg

# sacamos los promedios para estos valores
largo_promedio_aleta = np.mean(largo_aletas)
masa_promedio_pinguinos = np.mean(masa_pinguinos)





# Primero inicializamos el numerador y denominador para el parámetro a, que luego modificaremos en un loop for
numerador = 0 ;
denominador = 0 ;

for i in range(len(largo_aletas)):
  numerador += (largo_aletas[i]-largo_promedio_aleta)*(masa_pinguinos[i]-masa_promedio_pinguinos)
  denominador += (largo_aletas[i]-largo_promedio_aleta)**2 # El operador ** es el 'elevado a' de Python

a = numerador / denominador
b = masa_promedio_pinguinos - a * largo_promedio_aleta

print(a,b)





[(x,y) for x,y in zip(x,y)]


a


b


x = np.linspace(np.min(largo_aletas), np.max(largo_aletas), 1000)
y = a*x + b

fig, ax = plt.subplots()
#Graficamos nuestos puntos
ax.scatter(largo_aletas, masa_pinguinos, alpha=0.5,s=5, label='Datos') #seteamos la opacidad (alpha) y el tamaño (s) de los puntos ya que muchos se solapan y de esta forma podemos distinguirlos mejor
#Graficamos la regresión lineal
plt.plot(x,y,color='red', label='Regresión lineal')
#Seteamos los nombres de los ejes
ax.set_xlabel('Largo de aleta (mm)')
ax.set_ylabel('Masa de pingüinos (g)')
#Le decimos que muestre la leyenda y por último que grafique
ax.legend()
plt.show()






#Calculemos el RMSE
suma_cuadrados = 0
N = len(largo_aletas)

for i in range(N):
  y_pred = a*largo_aletas[i] + b
  suma_cuadrados += (largo_aletas[i]-y_pred)**2

RMSE = np.sqrt(suma_cuadrados/N)

print('RMSE: ' + str(RMSE))





#Calculemos R^2
numerador = 0
denominador = 0

for i in range(N):
  y_pred = a*largo_aletas[i] + b
  numerador += (y_pred-masa_promedio_pinguinos)**2
  denominador += (masa_pinguinos[i]-masa_promedio_pinguinos)**2

R2 = (numerador/denominador)
print('R^2: ' + str(R2))





import numpy as np
import matplotlib.pyplot as plt

X = np.asarray(largo_aletas, dtype=float)
y = np.asarray(masa_pinguinos, dtype=float)

# Parámetros iniciales
a, b = 0.0, 0.0

# Pasos (arrancá chico y dejá que se adapten)
step_a = 0.01
step_b = 0.01

# Hiperparámetros
max_steps = 5000
grow   = 1.01     # crecer suave si mejora
shrink = 0.99     # invertir y achicar si no mejora
patience = 300   # nº de pasos seguidos sin mejora antes de cortar
min_step_a = 1e-6
min_step_b = 1e-6
eps_improve = 0.0  # exige mejora estricta (podés poner 1e-15 si querés)

N = len(X)
def mse_params(a_, b_):
    err = (a_*X + b_) - y
    return float(np.dot(err, err) / N)

# Historial
hist = {"step": [], "a": [], "b": [], "mse": []}

mse = mse_params(a, b)

# 1) Elegir signo inicial del paso por tanteo local (sin gradientes)
if mse_params(a + step_a, b) > mse_params(a - step_a, b):
    step_a = -step_a
if mse_params(a, b + step_b) > mse_params(a, b - step_b):
    step_b = -step_b

no_improve = 0

for step in range(max_steps):
    improved = False

    # --- actualizar 'a' ---
    mse_curr = mse
    mse_try = mse_params(a + step_a, b)
    if mse_try < mse_curr - eps_improve:
        a = a + step_a
        mse = mse_try
        step_a *= grow
        improved = True
    else:
        step_a = -step_a * shrink
        mse_try2 = mse_params(a + step_a, b)
        if mse_try2 < mse_curr - eps_improve:
            a = a + step_a
            mse = mse_try2
            step_a *= grow
            improved = True

    # --- actualizar 'b' ---
    mse_curr = mse
    mse_try = mse_params(a, b + step_b)
    if mse_try < mse_curr - eps_improve:
        b = b + step_b
        mse = mse_try
        step_b *= grow
        improved = True
    else:
        step_b = -step_b * shrink
        mse_try2 = mse_params(a, b + step_b)
        if mse_try2 < mse_curr - eps_improve:
            b = b + step_b
            mse = mse_try2
            step_b *= grow
            improved = True

    # Log y tracking
    if step % 500 == 0:
        print(f"step={step:5d}  a={a: .6f}  b={b: .6f}  mse={mse: .6f}")

    hist["step"].append(step)
    hist["a"].append(a)
    hist["b"].append(b)
    hist["mse"].append(mse)

    # 2) Criterio de parada robusto: paciencia + paso mínimo
    if improved:
        no_improve = 0
    else:
        no_improve += 1

    if (no_improve >= patience) or (abs(step_a) < min_step_a and abs(step_b) < min_step_b):
        print(f"Paro en step {step}: no mejora en {no_improve} pasos seguidos "
              f"o steps mínimos alcanzados (|step_a|={abs(step_a):.2e}, |step_b|={abs(step_b):.2e}).")
        break

print(f"\nFinal: a={a:.6f}, b={b:.6f}, MSE={mse:.6f}, RMSE={np.sqrt(mse):.6f}")

# Visualización
idx = np.argsort(X)
plt.scatter(X, y, alpha=0.6, label="Datos")
plt.plot(X[idx], (a*X[idx] + b), linewidth=2, label="Recta aprendida (coord. search)")
plt.legend()
plt.xlabel("X")
plt.ylabel("y")
plt.title("Ajuste lineal: tanteo por coordenadas con pasos adaptativos")
plt.show()

plt.plot(hist["step"][0:50], hist["mse"][0:50])
plt.xlabel("step")
plt.ylabel("MSE")
plt.title("Progreso del error (MSE)")
plt.show()


# Primeras 30 regresiones (amarillo -> rojo) sobre los datos
n = min(30, len(hist["a"]))            # por si hay menos de 10 pasos
xs = np.linspace(X.min(), X.max(), 200)

cmap = plt.get_cmap("YlOrRd")
norm = plt.Normalize(vmin=0, vmax=max(1, n-1))

fig, ax = plt.subplots(figsize=(8, 5))
ax.scatter(X, y, s=20, alpha=0.6, label="Datos")

for i in range(n):
    ai = hist["a"][i]
    bi = hist["b"][i]
    ax.plot(xs, ai*xs + bi, lw=2, alpha=0.95, color=cmap(norm(i)))

ax.set_xlabel('Largo de aleta (mm)')
ax.set_ylabel('Masa de pingüinos (g)')
ax.set_title("Primeras 30 regresiones (amarillo→rojo) sobre los datos")
ax.legend()
plt.show()


# Primeras 30 regresiones (amarillo -> rojo) sobre los datos
n = min(1000, len(hist["a"]))            # por si hay menos de 10 pasos
xs = np.linspace(X.min(), X.max(), 200)

cmap = plt.get_cmap("YlOrRd")
norm = plt.Normalize(vmin=0, vmax=max(1, n-1))

fig, ax = plt.subplots(figsize=(8, 5))
ax.scatter(X, y, s=20, alpha=0.6, label="Datos")

for i in range(n):
    ai = hist["a"][i]
    bi = hist["b"][i]
    ax.plot(xs, ai*xs + bi, lw=2, alpha=0.95, color=cmap(norm(i)))

ax.set_xlabel('Largo de aleta (mm)')
ax.set_ylabel('Masa de pingüinos (g)')
ax.set_title("Primeras 1000 regresiones (amarillo→rojo) sobre los datos")
ax.legend()
plt.show()








import numpy as np














# importando librerias
import torch, random


# We can construct a tensor directly from some common python iterables,
# such as list and tuple nested iterables can also be handled as long as the
# dimensions are compatible

# tensor from a list
a = torch.tensor([0, 1, 2])

#tensor from a tuple of tuples
b = ((1.0, 1.1), (1.2, 1.3))
b = torch.tensor(b)

# tensor from a numpy array
c = np.ones([2, 3])
c = torch.tensor(c)

print(f"Tensor a: {a}")
print(f"Tensor b: {b}")
print(f"Tensor c: {c}")





# The numerical arguments we pass to these constructors
# determine the shape of the output tensor

x = torch.ones(5, 3)
y = torch.zeros(2)
z = torch.empty(1, 1, 5)
print(f"Tensor x: {x}")
print(f"Tensor y: {y}")
print(f"Tensor z: {z}")





# @title Video 7: Getting Data
from ipywidgets import widgets
from IPython.display import YouTubeVideo
from IPython.display import IFrame
from IPython.display import display


class PlayVideo(IFrame):
  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):
    self.id = id
    if source == 'Bilibili':
      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'
    elif source == 'Osf':
      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'
    super(PlayVideo, self).__init__(src, width, height, **kwargs)


def display_videos(video_ids, W=400, H=300, fs=1):
  tab_contents = []
  for i, video_id in enumerate(video_ids):
    out = widgets.Output()
    with out:
      if video_ids[i][0] == 'Youtube':
        video = YouTubeVideo(id=video_ids[i][1], width=W,
                             height=H, fs=fs, rel=0)
        print(f'Video available at https://youtube.com/watch?v={video.id}')
      else:
        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,
                          height=H, fs=fs, autoplay=False)
        if video_ids[i][0] == 'Bilibili':
          print(f'Video available at https://www.bilibili.com/video/{video.id}')
        elif video_ids[i][0] == 'Osf':
          print(f'Video available at https://osf.io/{video.id}')
      display(video)
    tab_contents.append(out)
  return tab_contents


video_ids = [('Youtube', 'LSkjPM1gFu0'), ('Bilibili', 'BV1744y127SQ')]
tab_contents = display_videos(video_ids, W=854, H=480)
tabs = widgets.Tab()
tabs.children = tab_contents
for i in range(len(tab_contents)):
  tabs.set_title(i, video_ids[i][0])
display(tabs)


# @title Submit your feedback
content_review(f"{feedback_prefix}_Getting_Data_Video")





# Import dataset and dataloaders related packages
from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, Grayscale





# Download and load the images from the CIFAR10 dataset
cifar10_data = datasets.CIFAR10(
    root="data",  # path where the images will be stored
    download=True,  # all images should be downloaded
    transform=ToTensor()  # transform the images to tensors
    )

# Print the number of samples in the loaded dataset
print(f"Number of samples: {len(cifar10_data)}")
print(f"Class names: {cifar10_data.classes}")





# Choose a random sample
random.seed(2021)
image, label = cifar10_data[random.randint(0, len(cifar10_data))]
print(f"Label: {cifar10_data.classes[label]}")
print(f"Image size: {image.shape}")





from typing import List, Tuple, Any, Dict
class PenguinDataset(Dataset):
    """Penguin dataset class.

    Parameters
    ----------
    input_keys : List[str]
        The column titles to use in the input feature vectors.
    target_key : str
        The column titles to use in the target feature vectors.
    train : bool
        If ``True``, this object will serve as the training set, and if
        ``False``, the validation set.

    Notes
    -----
    The validation split contains 10 male and 10 female penguins of each
    species.

    """

    def __init__(
        self,
        input_keys: List[str],
        target_key: str,
        train: bool,
    ):
        """Build ``PenguinDataset``."""
        self.input_keys = input_keys
        self.target_key = target_key

        data = load_penguins()
        data = (
            data.loc[~data.isna().any(axis=1)]
            .sort_values(by=sorted(data.keys()))
            .reset_index(drop=True)
        )
        # Transform the sex field into a float, with male represented by 1.0, female by 0.0
        data.sex = (data.sex == "male").astype(float)
        self.full_df = data

        valid_df = self.full_df.groupby(by=["species", "sex"]).sample(
            n=10,
            random_state=123,
        )
        # The training items are simply the items *not* in the valid split
        train_df = self.full_df.loc[~self.full_df.index.isin(valid_df.index)]

        self.split = {"train": train_df, "valid": valid_df}[
            "train" if train is True else "valid"
        ]

        # Build label map from the full dataset
        unique_labels = sorted(self.full_df[self.target_key].unique())
        self.label_map: Dict[str, int] = {
            label: idx for idx, label in enumerate(unique_labels)
        }

    def __len__(self) -> int:
        """Return the length of requested split.

        Returns
        -------
        int
            The number of items in the dataset.

        """
        return len(self.split)

    def __getitem__(self, idx: int) -> Tuple[Any, Any]:
        """Return an input-target pair.

        Parameters
        ----------
        idx : int
            Index of the input-target pair to return.

        Returns
        -------
        in_feats : Any
            Inputs.
        target : Any
            Targets.

        """
        # get the row index (idx) from the dataframe and
        # select relevant column features (provided as input_keys)
        feats = tuple(self.split.iloc[idx][self.input_keys])

        # this gives a 'species' i.e. one of ('Gentoo',), ('Chinstrap',), or ('Adelie',)
        tgt = self.split.iloc[idx][self.target_key]

        # Task 4 -- Part (a): Convert the tuple features to PyTorch Tensors

        # Task 4 -- Part (b): Convert the target (a Python integer) to a 0-D tensor (scalar tensor).


        return feats, tgt


from torch.utils.data import DataLoader

# Create training and validation DataLoaders.











# Load the training samples
training_data = datasets.CIFAR10(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
    )

# Load the test samples
test_data = datasets.CIFAR10(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
    )








# Create dataloaders with
train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)











# Load the next batch
batch_images, batch_labels = next(iter(train_dataloader))
print('Batch size:', batch_images.shape)

# Display the first image from the batch
plt.imshow(batch_images[0].permute(1, 2, 0))
plt.show()








from torch.nn import Module
from torch.nn import BatchNorm1d, Linear, ReLU, Dropout
from torch import Tensor


class FCNet(Module):
    """Fully-connected neural network."""

    # define __init__ function - model defined here.
    def __init__(self):
        pass

    # define forward function which calls network
    def forward(self, batch: Tensor) -> Tensor:
        pass


# define a model and print and test (try with torch.rand() function)





from torch.nn import CrossEntropyLoss


## aca nosotros podriamos jugar con cosas que no sean MSE, que es el loss que usa el SGDRegressor de sklearn





# Create an optimiser and give it the model's parameters.
from torch.optim import Adam





from typing import Dict


def train_one_epoch(
    model: Module,
    train_loader: DataLoader,
    optimiser: Adam,
    loss_func: CrossEntropyLoss,
) -> Dict[str, float]:
    """Train ``model`` for once epoch.

    Parameters
    ----------
    model : Module
        The neural network.
    train_loader : DataLoader
        Training dataloader.
    optimiser : Adam
        The optimiser.
    loss_func : CrossEntropyLoss
        Cross-entropy loss function.

    Returns
    -------
    Dict[str, float]
        A dictionary of metrics.

    """

    # setup the model for training. IMPORTANT!

    # setup loss and accuracy metrics dictionary

    # iterate over the batch, targets in the train_loader
    for batch, targets in train_loader:
        pass

        # zero the gradients (otherwise gradients accumulate)

        # run forward model to make predictions

        # compute loss
        # e.g. pred : Tensor([3]) and target : int

        # compute gradients

        # nudge parameters in direction of steepest descent

        # append metrics


def validate_one_epoch(
    model: Module,
    valid_loader: DataLoader,
    loss_func: CrossEntropyLoss,
) -> Dict[str, float]:
    """Validate ``model`` for a single epoch.

    Parameters
    ----------
    model : Module
        The neural network.
    valid_loader : DataLoader
        Validation dataloader.
    loss_func : CrossEntropyLoss
        Cross-entropy loss function.

    Returns
    -------
    Dict[str, float]
        Metrics of interest.

    """

    for batch, targets in valid_loader:
        pass





epochs = 3

# define train_metrics and valid_metrics lists.

for _ in range(epochs):

    # append output of train_one_epoch() to train_metrics

    # append output of valid_one_epoch() to valid_metrics

    pass





import matplotlib.pyplot as plt

quantities = ["loss", "accuracy"]
splits = ["train", "valid"]



# la que sigue abajo es de otro notebook que es de NeuroMatch, capaz borrar, 


class WideNet(nn.Module):
  """
   A Wide neural network with a single hidden layer
   Structure is as follows:
   nn.Sequential(
        nn.Linear(1, n_cells) + nn.Tanh(), # Fully connected layer with tanh activation
        nn.Linear(n_cells, 1) # Final fully connected layer
    )
  """

  def __init__(self):
    """
    Initializing the parameters of WideNet

    Args:
      None

    Returns:
      Nothing
    """
    n_cells = 512
    super().__init__()
    self.layers = nn.Sequential(
        nn.Linear(1, n_cells),
        nn.Tanh(),
        nn.Linear(n_cells, 1),
    )

  def forward(self, x):
    """
    Forward pass of WideNet

    Args:
      x: torch.Tensor
        2D tensor of features

    Returns:
      Torch tensor of model predictions
    """
    return self.layers(x)


# Creating an instance
set_seed(seed=42)
wide_net = WideNet()
print(wide_net)


# Create a mse loss function
loss_function = nn.MSELoss()

# Stochstic Gradient Descent optimizer (you will learn about momentum soon)
lr = 0.003  # Learning rate
sgd_optimizer = torch.optim.SGD(wide_net.parameters(), lr=lr, momentum=0.9)





import torch
import pandas as pd
import numpy as np

# Filtrar filas completas
cols = ["flipper_length_mm", "body_mass_g"]
data = data[cols].dropna()

# Tensores float32, forma (N, 1)
x = torch.tensor(data["flipper_length_mm"].to_numpy(), dtype=torch.float32).view(-1, 1)
y = torch.tensor(data["body_mass_g"].to_numpy(), dtype=torch.float32).view(-1, 1)

# Normalización (opcional pero recomendado para estabilidad numérica)
x_mean, x_std = x.mean(), x.std()
y_mean, y_std = y.mean(), y.std()

x_n = (x - x_mean) / (x_std + 1e-8)
y_n = (y - y_mean) / (y_std + 1e-8)


import torch

# --- Preparar tensores desde tu DataFrame df ---
cols = ["flipper_length_mm", "body_mass_g"]
data = data[cols].dropna()

x = torch.tensor(data["flipper_length_mm"].to_numpy(), dtype=torch.float32).view(-1, 1)
y = torch.tensor(data["body_mass_g"].to_numpy(), dtype=torch.float32).view(-1, 1)

# Normalización (opcional pero ayuda a estabilidad)
x_mean, x_std = x.mean(), x.std()
y_mean, y_std = y.mean(), y.std()
x_n = (x - x_mean) / (x_std + 1e-8)
y_n = (y - y_mean) / (y_std + 1e-8)

# --- Parámetros del modelo (escala normalizada) ---
torch.manual_seed(0)
w = torch.zeros(1, 1, requires_grad=True)   # pendiente
b = torch.zeros(1,    requires_grad=True)   # intercepto

lr = 0.05
loss_fn = torch.nn.MSELoss()
loss_history = []

for epoch in range(2000):
    # forward
    y_hat_n = x_n @ w + b
    loss = loss_fn(y_hat_n, y_n)

    # backward
    loss.backward()
    loss_history.append(loss.item())
    # update manual (sin torch.optim)
    with torch.no_grad():
        w -= lr * w.grad
        b -= lr * b.grad
        w.grad.zero_()
        b.grad.zero_()

    if epoch % 200 == 0:
        print(f"Epoch {epoch:4d} | loss={loss.item():.6f}")

# --- Volver a escala original y métricas ---
with torch.no_grad():
    w_n = w.item()
    b_n = b.item()
    slope = (y_std * w_n / (x_std + 1e-8)).item()
    intercept = (y_std * (b_n - w_n * x_mean / (x_std + 1e-8)) + y_mean).item()

    y_pred = (y_hat_n * y_std) + y_mean
    sse = torch.sum((y - y_pred) ** 2)
    sst = torch.sum((y - y.mean()) ** 2)
    r2 = (1 - sse / sst).item()

print(f"Pendiente (g por mm): {slope:.3f}")
print(f"Intercepto (g):       {intercept:.3f}")
print(f"R^2:                  {r2:.4f}")


import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(loss_history, label="Training MSE (normalizado)")
plt.xlabel("Época")
plt.ylabel("Loss")
plt.title("Curva de entrenamiento")
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

# (Opcional) misma curva en escala logarítmica para ver mejor la convergencia
plt.figure(figsize=(8,5))
plt.plot(loss_history, label="Training MSE (normalizado)")
plt.yscale("log")
plt.xlabel("Época")
plt.ylabel("Loss (log)")
plt.title("Curva de entrenamiento (escala log)")
plt.grid(True, which="both", alpha=0.3)
plt.legend()
plt.show()



